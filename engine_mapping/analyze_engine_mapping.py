#!/usr/bin/env python3
"""
analyze_engine_mapping.py - Neuron Profile JSON ë¶„ì„

ë‹¨ì¼ ì»¤ë„ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ì—ì„œ ê° ì—°ì‚°ì´ ì–´ë–¤ ì—”ì§„(ë“¤)ì„ ì‚¬ìš©í•˜ëŠ”ì§€ íŒŒì•….
"""

import json
import os
import sys
import csv
from datetime import datetime

# ============================================================
# ì„¤ì •: ì—¬ê¸°ì„œ ì…ë ¥/ì¶œë ¥ ì§€ì •
# ============================================================
INPUT_DIR = "/Users/parkjuhyun/Desktop/project_chip/npu_profiling_code/single_kernel/results"
OUTPUT_DIR = "/Users/parkjuhyun/Desktop/project_chip/npu_profiling_code/engine_mapping/results"
# ============================================================


def load_profile(json_path):
    """JSON í”„ë¡œíŒŒì¼ ë¡œë“œ."""
    with open(json_path, 'r') as f:
        return json.load(f)


def extract_engine_summary(data):
    """summaryì—ì„œ ì—”ì§„ë³„ ì •ë³´ ì¶”ì¶œ."""
    s = data['summary'][0]
    
    return {
        # Active time (seconds)
        'tensor_engine_time_us':  s.get('tensor_engine_active_time', 0) * 1e6,
        'vector_engine_time_us':  s.get('vector_engine_active_time', 0) * 1e6,
        'scalar_engine_time_us':  s.get('scalar_engine_active_time', 0) * 1e6,
        'gpsimd_engine_time_us':  s.get('gpsimd_engine_active_time', 0) * 1e6,
        'sync_engine_time_us':    s.get('sync_engine_active_time', 0) * 1e6,
        'dma_time_us':            s.get('dma_active_time', 0) * 1e6,
        'total_time_us':          s.get('total_time', 0) * 1e6,
        'total_active_time_us':   s.get('total_active_time', 0) * 1e6,
        
        # Active time percent
        'tensor_engine_pct':  s.get('tensor_engine_active_time_percent', 0) * 100,
        'vector_engine_pct':  s.get('vector_engine_active_time_percent', 0) * 100,
        'scalar_engine_pct':  s.get('scalar_engine_active_time_percent', 0) * 100,
        'gpsimd_engine_pct':  s.get('gpsimd_engine_active_time_percent', 0) * 100,
        'dma_pct':            s.get('dma_active_time_percent', 0) * 100,
        
        # Instruction counts
        'tensor_instr_count': s.get('tensor_engine_instruction_count', 0),
        'vector_instr_count': s.get('vector_engine_instruction_count', 0),
        'scalar_instr_count': s.get('scalar_engine_instruction_count', 0),
        'gpsimd_instr_count': s.get('gpsimd_engine_instruction_count', 0),
        'sync_instr_count':   s.get('sync_engine_instruction_count', 0),
        
        # Instruction time (seconds â†’ us)
        'tensor_instr_time_us': s.get('tensor_engine_instruction_time', 0) * 1e6,
        'vector_instr_time_us': s.get('vector_engine_instruction_time', 0) * 1e6,
        'scalar_instr_time_us': s.get('scalar_engine_instruction_time', 0) * 1e6,
        'gpsimd_instr_time_us': s.get('gpsimd_engine_instruction_time', 0) * 1e6,
        
        # Memory
        'sbuf_read_bytes':  s.get('sbuf_read_bytes', 0),
        'sbuf_write_bytes': s.get('sbuf_write_bytes', 0),
        'hbm_read_bytes':   s.get('hbm_read_bytes', 0),
        'hbm_write_bytes':  s.get('hbm_write_bytes', 0),
        
        # Model info
        'model_flops':      s.get('model_flops', 0),
    }


def classify_engines(info, threshold_pct=1.0):
    """ì–´ë–¤ ì—”ì§„ì„ 'ì‹¤ì§ˆì ìœ¼ë¡œ' ì‚¬ìš©í•˜ëŠ”ì§€ ë¶„ë¥˜.
    
    Args:
        info: extract_engine_summary ê²°ê³¼
        threshold_pct: ì´ ë¹„ìœ¨(%) ì´ìƒì´ë©´ 'ì‚¬ìš©'ìœ¼ë¡œ íŒë‹¨
    """
    engines = []
    
    if info['tensor_engine_pct'] >= threshold_pct:
        engines.append('TensorEngine')
    if info['vector_engine_pct'] >= threshold_pct:
        engines.append('VectorEngine')
    if info['scalar_engine_pct'] >= threshold_pct:
        engines.append('ScalarEngine')
    if info['gpsimd_engine_pct'] >= threshold_pct:
        engines.append('GPSIMD')
    
    return engines


def extract_instruction_types(data):
    """instruction ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³ ìœ í•œ instruction type ì¶”ì¶œ."""
    instructions = data.get('instruction', [])
    
    type_counts = {}
    for inst in instructions:
        itype = inst.get('instruction_type', 'unknown')
        opcode = inst.get('opcode', 'unknown')
        key = f"{itype}"
        type_counts[key] = type_counts.get(key, 0) + 1
    
    return type_counts


def print_report(profile_dir):
    """í”„ë¡œíŒŒì¼ ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥."""
    
    json_files = sorted([f for f in os.listdir(profile_dir) if f.endswith('.json')])
    
    if not json_files:
        print(f"No JSON files found in {profile_dir}")
        return
    
    results = {}
    
    for fname in json_files:
        # ì—°ì‚° ì´ë¦„ ì¶”ì¶œ: profile_add_4096x4096.json â†’ add
        op_name = fname.replace('profile_', '').replace('_4096x4096.json', '').replace('.json', '')
        
        data = load_profile(os.path.join(profile_dir, fname))
        info = extract_engine_summary(data)
        engines_used = classify_engines(info)
        instr_types = extract_instruction_types(data)
        
        results[op_name] = {
            'info': info,
            'engines': engines_used,
            'instr_types': instr_types,
        }
    
    # ============================================================
    # Report 1: ì—”ì§„ ì‚¬ìš© ì‹œê°„ (ì ˆëŒ€ê°’)
    # ============================================================
    print("=" * 110)
    print("ğŸ“Š ì—”ì§„ë³„ Active Time (Î¼s)")
    print("=" * 110)
    print(f"{'Operation':<12} {'TensorEng':>10} {'VectorEng':>10} {'ScalarEng':>10} {'GPSIMD':>10} {'DMA':>10} {'Total':>10}")
    print("-" * 110)
    
    for op, r in sorted(results.items()):
        i = r['info']
        print(f"{op:<12} {i['tensor_engine_time_us']:>10.1f} {i['vector_engine_time_us']:>10.1f} "
              f"{i['scalar_engine_time_us']:>10.1f} {i['gpsimd_engine_time_us']:>10.1f} "
              f"{i['dma_time_us']:>10.1f} {i['total_time_us']:>10.1f}")
    
    # ============================================================
    # Report 2: ì—”ì§„ ì‚¬ìš© ë¹„ìœ¨ (%)
    # ============================================================
    print()
    print("=" * 90)
    print("ğŸ“Š ì—”ì§„ë³„ Active Time (%)")
    print("=" * 90)
    print(f"{'Operation':<12} {'TensorEng':>10} {'VectorEng':>10} {'ScalarEng':>10} {'GPSIMD':>10} {'DMA':>10}")
    print("-" * 90)
    
    for op, r in sorted(results.items()):
        i = r['info']
        print(f"{op:<12} {i['tensor_engine_pct']:>9.1f}% {i['vector_engine_pct']:>9.1f}% "
              f"{i['scalar_engine_pct']:>9.1f}% {i['gpsimd_engine_pct']:>9.1f}% "
              f"{i['dma_pct']:>9.1f}%")
    
    # ============================================================
    # Report 3: Instruction Count
    # ============================================================
    print()
    print("=" * 90)
    print("ğŸ“Š ì—”ì§„ë³„ Instruction Count")
    print("=" * 90)
    print(f"{'Operation':<12} {'TensorEng':>10} {'VectorEng':>10} {'ScalarEng':>10} {'GPSIMD':>10} {'Sync':>10}")
    print("-" * 90)
    
    for op, r in sorted(results.items()):
        i = r['info']
        print(f"{op:<12} {i['tensor_instr_count']:>10} {i['vector_instr_count']:>10} "
              f"{i['scalar_instr_count']:>10} {i['gpsimd_instr_count']:>10} "
              f"{i['sync_instr_count']:>10}")
    
    # ============================================================
    # Report 4: ì—”ì§„ ë¶„ë¥˜ ê²°ê³¼ (í•µì‹¬!)
    # ============================================================
    print()
    print("=" * 90)
    print("ğŸ”¬ ì—°ì‚°ë³„ ì—”ì§„ ë¶„ë¥˜ (active time > 1% ê¸°ì¤€)")
    print("=" * 90)
    
    for op, r in sorted(results.items()):
        engines = r['engines']
        is_single = len(engines) == 1
        marker = "âœ… ë‹¨ì¼ ì—”ì§„" if is_single else "âš ï¸ ë³µí•© ì—”ì§„"
        print(f"  {op:<12} â†’ {', '.join(engines):<40} {marker}")
    
    # ============================================================
    # Report 5: Cross-Engine ì‹¤í—˜ ì í•©ì„±
    # ============================================================
    print()
    print("=" * 90)
    print("ğŸ¯ Cross-Engine ì „í™˜ ë¹„ìš© ì¸¡ì •ì— ì í•©í•œ ì—°ì‚°")
    print("=" * 90)
    
    single_tensor = []
    single_vector = []
    single_scalar = []
    multi_engine = []
    
    for op, r in sorted(results.items()):
        engines = r['engines']
        if len(engines) == 1:
            if 'TensorEngine' in engines:
                single_tensor.append(op)
            elif 'VectorEngine' in engines:
                single_vector.append(op)
            elif 'ScalarEngine' in engines:
                single_scalar.append(op)
        else:
            multi_engine.append((op, engines))
    
    print(f"\n  âœ… TensorEngine ë‹¨ì¼: {', '.join(single_tensor) if single_tensor else 'ì—†ìŒ'}")
    print(f"  âœ… VectorEngine ë‹¨ì¼: {', '.join(single_vector) if single_vector else 'ì—†ìŒ'}")
    print(f"  âœ… ScalarEngine ë‹¨ì¼: {', '.join(single_scalar) if single_scalar else 'ì—†ìŒ'}")
    print(f"\n  âš ï¸ ë³µí•© ì—”ì§„ (cross-engine ì¸¡ì •ì— ë¶€ì í•©):")
    for op, engines in multi_engine:
        print(f"     {op}: {', '.join(engines)}")
    
    # ============================================================
    # Report 6: Memory Usage
    # ============================================================
    print()
    print("=" * 90)
    print("ğŸ“Š Memory Access (bytes)")
    print("=" * 90)
    print(f"{'Operation':<12} {'SBUF Read':>14} {'SBUF Write':>14} {'HBM Read':>14} {'HBM Write':>14}")
    print("-" * 90)
    
    for op, r in sorted(results.items()):
        i = r['info']
        print(f"{op:<12} {i['sbuf_read_bytes']:>14,} {i['sbuf_write_bytes']:>14,} "
              f"{i['hbm_read_bytes']:>14,} {i['hbm_write_bytes']:>14,}")
    
    # ============================================================
    # Save CSV
    # ============================================================
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    csv_path = os.path.join(OUTPUT_DIR, f"engine_mapping_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    fieldnames = [
        'operation', 'engines_used', 'is_single_engine',
        'tensor_engine_time_us', 'vector_engine_time_us', 'scalar_engine_time_us',
        'gpsimd_engine_time_us', 'dma_time_us', 'total_time_us',
        'tensor_engine_pct', 'vector_engine_pct', 'scalar_engine_pct', 'gpsimd_engine_pct',
        'tensor_instr_count', 'vector_instr_count', 'scalar_instr_count', 'gpsimd_instr_count',
        'sbuf_read_bytes', 'sbuf_write_bytes', 'hbm_read_bytes', 'hbm_write_bytes',
        'model_flops',
    ]
    
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for op, r in sorted(results.items()):
            i = r['info']
            row = {
                'operation': op,
                'engines_used': '|'.join(r['engines']),
                'is_single_engine': len(r['engines']) == 1,
            }
            for k in fieldnames[3:]:
                row[k] = round(i.get(k, 0), 4)
            writer.writerow(row)
    
    print(f"\nğŸ“ CSV ì €ì¥: {csv_path}")
    
    return results


if __name__ == "__main__":
    print(f"Profile directory: {INPUT_DIR}\n")
    print_report(INPUT_DIR)
