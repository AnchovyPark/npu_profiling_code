#!/usr/bin/env python3
"""
cross_engine.py - Cross-Engine Transition Overhead Profiling (AWS Inferentia2)

NeuronCore-v2ì˜ ì—”ì§„ ê°„(TensorEngine â†” VectorEngine) ì „í™˜ ë¹„ìš©ì„ ì¸¡ì •.
ì¢…ì†ì (dependent) vs ë…ë¦½ì (independent) ì—°ì‚° ìŒì„ ë¹„êµí•˜ì—¬
ìˆœìˆ˜ ì „í™˜ ë¹„ìš©ê³¼ íŒŒì´í”„ë¼ì´ë‹ íš¨ê³¼ë¥¼ ë¶„ë¦¬.

LLM ì¶”ë¡  ê³¼ì •ì—ì„œ ì‹¤ì œ ë°œìƒí•˜ëŠ” 5ê°€ì§€ cross-engine ì¡°í•©:
1. MatMul â†” Add         (projection â†’ residual connection)
2. MatMul â†” GELU        (FFN up-projection â†’ activation)
3. MatMul â†” SiLU        (FFN gate-projection â†’ activation, LLaMA style)
4. MatMul â†” LayerNorm   (projection output â†’ normalization)
5. MatMul â†” Softmax     (QK^T â†’ softmax in attention)

ê° ì¡°í•©ì˜ ì •ë°©í–¥/ì—­ë°©í–¥ í¬í•¨ â†’ ì´ 10ê°€ì§€ ì¡°í•©
í…ì„œ í¬ê¸°: 2048 ~ 4096 (5ê°œ í¬ì¸íŠ¸)

Usage:
    python cross_engine.py
"""

import torch
import torch.nn as nn
import torch_neuronx
import time
import csv
import os
import numpy as np
from datetime import datetime


# ============================================================
# Configuration
# ============================================================
SIZES = [2048, 2560, 3072, 3584, 4096]
HIDDEN = 4096  # LLM hidden dimension (e.g., LLaMA 7B/8B)
ITERATIONS = 10
WARMUP = 3


# ============================================================
# FLOPs Calculation
# ============================================================
def calc_flops(op_name, M, H):
    """ì—°ì‚°ë³„ FLOPs ê³„ì‚°.
    
    Args:
        op_name: ì—°ì‚° ì´ë¦„
        M: batch Ã— seq_len (í–‰ ìˆ˜)
        H: hidden dimension (ì—´ ìˆ˜)
    """
    flops_map = {
        "matmul":    2 * M * H * H,    # (M, H) @ (H, H)
        "add":       M * H,            # element-wise add
        "gelu":      8 * M * H,        # x * 0.5 * (1 + tanh(...))
        "silu":      4 * M * H,        # x * sigmoid(x)
        "layernorm": 5 * M * H,        # mean, var, normalize, scale, shift
        "softmax":   5 * M * H,        # exp, sum, div
    }
    return flops_map[op_name]


# ============================================================
# Operation Modules
# ============================================================
class MatMulOp(nn.Module):
    """TensorEngine: í–‰ë ¬ê³± (H, H) weight"""
    def __init__(self, H):
        super().__init__()
        self.register_buffer('weight', torch.randn(H, H) * (2.0 / H) ** 0.5)

    def forward(self, x):
        return torch.matmul(x, self.weight)


class AddOp(nn.Module):
    """VectorEngine: element-wise add (bias)"""
    def __init__(self, H):
        super().__init__()
        self.register_buffer('bias', torch.randn(1, H) * 0.01)

    def forward(self, x):
        return x + self.bias


class GELUOp(nn.Module):
    """VectorEngine: GELU activation"""
    def forward(self, x):
        return torch.nn.functional.gelu(x)


class SiLUOp(nn.Module):
    """VectorEngine: SiLU activation (LLaMA)"""
    def forward(self, x):
        return torch.nn.functional.silu(x)


class LayerNormOp(nn.Module):
    """VectorEngine: Layer Normalization"""
    def __init__(self, H):
        super().__init__()
        self.ln = nn.LayerNorm(H)

    def forward(self, x):
        return self.ln(x)


class SoftmaxOp(nn.Module):
    """VectorEngine: Softmax"""
    def forward(self, x):
        return torch.nn.functional.softmax(x, dim=-1)


def make_op(name, H):
    """ì´ë¦„ìœ¼ë¡œ Operation module ìƒì„±."""
    builders = {
        "matmul":    lambda: MatMulOp(H),
        "add":       lambda: AddOp(H),
        "gelu":      lambda: GELUOp(),
        "silu":      lambda: SiLUOp(),
        "layernorm": lambda: LayerNormOp(H),
        "softmax":   lambda: SoftmaxOp(),
    }
    return builders[name]()


# ============================================================
# Pair Models (Dependent / Independent)
# ============================================================
class DependentPair(nn.Module):
    """ì¢…ì†ì  ì—°ì‚° ìŒ: op1ì˜ ì¶œë ¥ì´ op2ì˜ ì…ë ¥.
    
    op1 â†’ result â†’ op2 (sequential dependency)
    Cross-engine ì „í™˜ ë¹„ìš©ì´ latencyì— ì§ì ‘ ë°˜ì˜ë¨.
    """
    def __init__(self, op1, op2):
        super().__init__()
        self.op1 = op1
        self.op2 = op2

    def forward(self, x):
        y = self.op1(x)
        z = self.op2(y)
        return z.sum()  # ê²°ê³¼ ì‚¬ìš© (NPU ì—°ì‚° skip ë°©ì§€)


class IndependentPair(nn.Module):
    """ë…ë¦½ì  ì—°ì‚° ìŒ: ì„œë¡œ ë‹¤ë¥¸ ì…ë ¥, ì¢…ì†ì„± ì—†ìŒ.
    
    op1(x1), op2(x2) (no dependency)
    ì—”ì§„ ê°„ ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ â†’ ì „í™˜ ë¹„ìš©ì´ ìˆ¨ê²¨ì§ˆ ìˆ˜ ìˆìŒ.
    """
    def __init__(self, op1, op2):
        super().__init__()
        self.op1 = op1
        self.op2 = op2

    def forward(self, x1, x2):
        y = self.op1(x1)
        z = self.op2(x2)
        return y.sum() + z.sum()  # ì–‘ìª½ ê²°ê³¼ ëª¨ë‘ ì‚¬ìš©


# ============================================================
# Benchmarking
# ============================================================
def benchmark(traced_model, inputs, iterations=ITERATIONS, warmup=WARMUP):
    """Traced modelì˜ latency ì¸¡ì • (ms).
    
    Args:
        traced_model: torch_neuronx.traceë¡œ ì»´íŒŒì¼ëœ ëª¨ë¸
        inputs: ì…ë ¥ í…ì„œ tuple
        iterations: ì¸¡ì • ë°˜ë³µ íšŸìˆ˜
        warmup: ì›Œë°ì—… íšŸìˆ˜
    
    Returns:
        float: í‰ê·  latency (ms)
    """
    # Warmup
    for _ in range(warmup):
        result = traced_model(*inputs)
        _ = result.item()  # sync

    # Measure
    latencies = []
    for _ in range(iterations):
        start = time.perf_counter()
        result = traced_model(*inputs)
        _ = result.item()  # NPU ì—°ì‚° ì™„ë£Œ ë³´ì¥ (sync)
        end = time.perf_counter()
        latencies.append((end - start) * 1000.0)  # â†’ ms

    return np.mean(latencies)


# ============================================================
# LLM Cross-Engine Operation Pairs
# ============================================================
# (op1, op2) â€” ì‹¤ì œ LLM ì¶”ë¡ ì—ì„œ ë°œìƒí•˜ëŠ” cross-engine ì „í™˜
PAIRS = [
    ("matmul", "add"),        # projection â†’ residual add
    ("matmul", "gelu"),       # FFN up-projection â†’ GELU (GPT style)
    ("matmul", "silu"),       # FFN gate-projection â†’ SiLU (LLaMA style)
    ("matmul", "layernorm"),  # projection output â†’ layer normalization
    ("matmul", "softmax"),    # QK^T â†’ softmax (attention)
]

# Engine mapping (ì°¸ê³ ìš©)
ENGINE_MAP = {
    "matmul":    "TensorEngine",
    "add":       "VectorEngine",
    "gelu":      "VectorEngine",
    "silu":      "VectorEngine",
    "layernorm": "VectorEngine",
    "softmax":   "VectorEngine",
}


# ============================================================
# Main
# ============================================================
def main():
    results = []
    total_experiments = len(PAIRS) * 2 * len(SIZES)  # pairs Ã— directions Ã— sizes
    current = 0

    print("=" * 70)
    print("Cross-Engine Transition Overhead Profiling")
    print(f"  Sizes: {SIZES}")
    print(f"  Hidden dim: {HIDDEN}")
    print(f"  Iterations: {ITERATIONS} (warmup: {WARMUP})")
    print(f"  Pairs: {len(PAIRS)} Ã— 2 directions Ã— {len(SIZES)} sizes = {total_experiments}")
    print("=" * 70)

    for op1_base, op2_base in PAIRS:
        # ì •ë°©í–¥: op1 â†’ op2
        # ì—­ë°©í–¥: op2 â†’ op1
        for (first, second) in [(op1_base, op2_base), (op2_base, op1_base)]:
            for size in SIZES:
                current += 1
                direction = f"{first}({ENGINE_MAP[first]}) â†’ {second}({ENGINE_MAP[second]})"
                print(f"\n[{current}/{total_experiments}] {direction}, size={size}")

                # --- Dependent model ---
                dep_model = DependentPair(make_op(first, HIDDEN), make_op(second, HIDDEN))
                x_dep = torch.randn(size, HIDDEN)

                print(f"  Compiling dependent model...", end=" ", flush=True)
                try:
                    dep_traced = torch_neuronx.trace(dep_model, (x_dep,))
                    print("OK")
                except Exception as e:
                    print(f"FAILED: {e}")
                    continue

                print(f"  Benchmarking dependent...", end=" ", flush=True)
                dep_latency = benchmark(dep_traced, (x_dep,))
                print(f"{dep_latency:.3f} ms")

                # --- Independent model ---
                indep_model = IndependentPair(make_op(first, HIDDEN), make_op(second, HIDDEN))
                x1 = torch.randn(size, HIDDEN)
                x2 = torch.randn(size, HIDDEN)

                print(f"  Compiling independent model...", end=" ", flush=True)
                try:
                    indep_traced = torch_neuronx.trace(indep_model, (x1, x2))
                    print("OK")
                except Exception as e:
                    print(f"FAILED: {e}")
                    continue

                print(f"  Benchmarking independent...", end=" ", flush=True)
                indep_latency = benchmark(indep_traced, (x1, x2))
                print(f"{indep_latency:.3f} ms")

                # --- FLOPs ---
                flops_first = calc_flops(first, size, HIDDEN)
                flops_second = calc_flops(second, size, HIDDEN)

                # --- Summary ---
                diff = dep_latency - indep_latency
                print(f"  â†’ Dependent: {dep_latency:.3f} ms | Independent: {indep_latency:.3f} ms | Diff: {diff:+.3f} ms")

                results.append({
                    "op1": first,
                    "op1_flops": flops_first,
                    "op2": second,
                    "op2_flops": flops_second,
                    "size": size,
                    "dep_latency_ms": round(dep_latency, 4),
                    "indep_latency_ms": round(indep_latency, 4),
                })

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del dep_model, dep_traced, indep_model, indep_traced
                torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # ============================================================
    # Save CSV
    # ============================================================
    output_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        f"cross_engine_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )

    fieldnames = [
        "op1",              # ì‹œì‘ ì—°ì‚°
        "op1_flops",        # ì‹œì‘ ì—°ì‚°ì˜ FLOPs
        "op2",              # ë ì—°ì‚°
        "op2_flops",        # ë ì—°ì‚°ì˜ FLOPs
        "size",             # í…ì„œ í¬ê¸° (M dimension)
        "dep_latency_ms",   # ì¢…ì†ì  ì—°ì‚° latency (ms)
        "indep_latency_ms", # ë…ë¦½ì  ì—°ì‚° latency (ms)
    ]

    with open(output_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"\n{'=' * 70}")
    print(f"Results saved to: {output_path}")
    print(f"Total experiments: {len(results)}")
    print(f"{'=' * 70}")

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š Summary:")
    print(f"{'Op1':<12} {'Op2':<12} {'Size':>6} {'Dep(ms)':>10} {'Indep(ms)':>10} {'Diff(ms)':>10}")
    print("-" * 66)
    for r in results:
        diff = r["dep_latency_ms"] - r["indep_latency_ms"]
        print(f"{r['op1']:<12} {r['op2']:<12} {r['size']:>6} "
              f"{r['dep_latency_ms']:>10.3f} {r['indep_latency_ms']:>10.3f} {diff:>+10.3f}")


if __name__ == "__main__":
    main()
